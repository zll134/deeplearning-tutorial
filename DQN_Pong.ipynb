{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Pong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1PRs6gEwcPnLsAGwqO3TuLmSwLkmIxunC",
      "authorship_tag": "ABX9TyN3G9SiLrLw1UxBa8RBkKUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zll134/deeplearning-tutorial/blob/master/DQN_Pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixAWjcdSu8Xq",
        "colab_type": "text"
      },
      "source": [
        "### Gym.wrapper\n",
        "\n",
        "将环境打包封装\n",
        "\n",
        "将对Atari Pong游戏进行实现\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwMQE5gTkDHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import cv2\n",
        "import numpy as np \n",
        "import gym.spaces\n",
        "import collections\n",
        "\n",
        "#实现Fire按钮的实现\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "  def __init__(self,env=None):\n",
        "    super(FireResetEnv,self).__init__(env);\n",
        "    assert env.unwrapped.get_action_meanings()[1]==\"FIRE\"\n",
        "    assert len(env.unwrapped.get_action_meanings())>=3\n",
        "  def step(self,action):\n",
        "    return self.env.step(action)\n",
        "  def reset(self):\n",
        "    self.env.reset();\n",
        "    obs,_,done,_=self.env.step(1)\n",
        "    if done:\n",
        "      self.env.reset();\n",
        "    obs,_,done,_=self.env.step(2)\n",
        "    if done :\n",
        "      self.env.reset()\n",
        "    return obs\n",
        "\n",
        "#实现K帧的合并,\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "  \"\"\"值返回第 skip th 的帧\"\"\"\n",
        "  def __init__(self,env=None,skip=4):\n",
        "    super(MaxAndSkipEnv,self).__init__(env);\n",
        "    #使用最新的观察参数\n",
        "    self._obs_buffer=collections.deque(maxlen=2)\n",
        "    self._skip=skip\n",
        "  def step(self,action):\n",
        "    total_reward=0.0;\n",
        "    done=None;\n",
        "    for _ in range(self._skip):\n",
        "      obs,reward,done,info =self.env.step(action)\n",
        "      self._obs_buffer.append(obs)\n",
        "      total_reward+=reward\n",
        "      if done :\n",
        "        break\n",
        "    #输出两个连续帧中较大的那个\n",
        "    max_frame=np.max(np.stack(self._obs_buffer),axis=0)\n",
        "    return max_frame,total_reward,done,info\n",
        "  def reset(self):\n",
        "    self._obs_buffer.clear()\n",
        "    obs = self.env.reset()\n",
        "    self._obs_buffer.append(obs)\n",
        "    return obs\n",
        "\n",
        "#将210*160分辨率的仿真器转化为84*84*1的灰度图\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "  def __init__(self,env=None):\n",
        "    super(ProcessFrame84,self).__init__(env)\n",
        "    self.observation_space=gym.spaces.Box(low=0,high=255,shape=[84,84,1],dtype=np.int8)\n",
        "  def observation(self,obs):\n",
        "    return ProcessFrame84.process(obs)\n",
        "  @staticmethod\n",
        "  def process(frame):\n",
        "    if frame.size==210*160*3:\n",
        "      img=np.reshape(frame,[210,160,3]).astype(np.float32)\n",
        "    elif frame.size==250*160*3:\n",
        "      img=np.reshape(frame,[250,160,3]).astype(np.float32)\n",
        "    else :\n",
        "      assert False,\"Unknown Resolution\"\n",
        "    img=img[:,:,0]*0.29+img[:,:,1]*0.59+img[:,:,2]*0.11\n",
        "    resized_screen=cv2.resize(img,(84,110),interpolation=cv2.INTER_AREA)\n",
        "    x_t=resized_screen[18:102,:]\n",
        "    x_t=np.reshape(x_t,[84,84,1])\n",
        "    return x_t.astype(np.uint8)\n",
        "\n",
        "#对buffer进行处理\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "  def __init__(self,env,n_steps,dtype=np.float32):\n",
        "    super(BufferWrapper,self).__init__(env)\n",
        "    self.dtype=dtype\n",
        "    old_space=env.observation_space\n",
        "    self.observation_space=gym.spaces.Box(old_space.low.repeat(n_steps,axis=0),old_space.high.repeat(n_steps,axis=0),dtype=dtype)\n",
        "  def reset(self):\n",
        "    self.buffer=np.zeros_like(self.observation_space.low,dtype=self.dtype)\n",
        "    return self.observation(self.env.reset())\n",
        "  def observation(self,observation):\n",
        "    self.buffer[:-1]=self.buffer[1:]\n",
        "    self.buffer[-1] = observation\n",
        "    return self.buffer\n",
        "\n",
        "#将图片转化为pytorch 需要的格式\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super(ImageToPyTorch, self).__init__(env)\n",
        "    old_shape = self.observation_space.shape\n",
        "    self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "    shape=(old_shape[-1], old_shape[0], old_shape[1]),dtype=np.float32)\n",
        "  def observation(self, observation):\n",
        "    return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "  def observation(self, obs):\n",
        "    return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "  env = gym.make(env_name)\n",
        "  env = MaxAndSkipEnv(env)\n",
        "  env = FireResetEnv(env)\n",
        "  env = ProcessFrame84(env)\n",
        "  env = ImageToPyTorch(env)\n",
        "  env = BufferWrapper(env, 4)\n",
        "  return ScaledFloatFrame(env)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e2b77Vh6Bry",
        "colab_type": "text"
      },
      "source": [
        "### DQN 模型\n",
        "\n",
        "神经网络是三个卷积层加上两个全连接层"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sgrkzr_6Eq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "import collections\n",
        "\n",
        "#DQN 模型\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self,input_shape,n_actions):\n",
        "    super(DQN,self).__init__();\n",
        "    self.conv=nn.Sequential(\n",
        "        nn.Conv2d(input_shape[0],32,kernel_size=8,stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32,64,kernel_size=4,stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64,64,kernel_size=3,stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    \n",
        "    conv_out_size=self.get_conv_out(input_shape)\n",
        "    self.fc=nn.Sequential(\n",
        "        nn.Linear(conv_out_size,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,n_actions)\n",
        "    )\n",
        "  def get_conv_out(self,shape):\n",
        "    o=self.conv(torch.zeros(1,*shape))\n",
        "    return int(np.prod(o.size()))\n",
        "  def forward(self,x):\n",
        "    conv_out=self.conv(x).view(x.size()[0],-1)\n",
        "    return self.fc(conv_out)\n",
        "\n",
        "#超参数\n",
        "gamma=0.99\n",
        "\n",
        "#rerplay buffer的代码\n",
        "Experience=collections.namedtuple('Experience',field_names=['state','action','reward','done','new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self,capacity):\n",
        "    self.buffer=collections.deque(maxlen=capacity)\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  def append(self,experience):\n",
        "    self.buffer.append(experience)\n",
        "  def sample(self,batch_size):\n",
        "    indices=np.random.choice(len(self.buffer),batch_size,replace=False)\n",
        "    states,actions,rewards,dones,next_states=zip(*[self.buffer[idx] for idx in indices])\n",
        "    return np.array(states),np.array(actions),np.array(rewards, dtype=np.float32),np.array(dones, dtype=np.int8),np.array(next_states)\n",
        "\n",
        "#与环境相互作用的agent\n",
        "class Agent:\n",
        "  def __init__(self,env,exp_buffer):\n",
        "    self.env=env;\n",
        "    self.exp_buffer=exp_buffer\n",
        "    self._reset();\n",
        "  def _reset(self):\n",
        "    self.state=self.env.reset()\n",
        "    self.total_reward=0.0\n",
        "  def play_step(self,net,epsilon=0.0,device='cpu'):\n",
        "    done_reward=None\n",
        "\n",
        "    #epsilon-greedy算法\n",
        "    if np.random.random()<epsilon:\n",
        "      action=env.action_space.sample()\n",
        "    else:\n",
        "      state_a=np.array([self.state],copy=False)\n",
        "      state_v=torch.tensor(state_a).to(device)\n",
        "      q_vals_v=net(state_v)\n",
        "      _,action_v=torch.max(q_vals_v,dim=1)\n",
        "      action=int(action_v.item())\n",
        "\n",
        "    #产生动作\n",
        "    new_state,reward,is_done,_=self.env.step(action)\n",
        "    self.total_reward+=reward\n",
        "    exp=Experience(self.state,action,reward,is_done,new_state)\n",
        "    self.exp_buffer.append(exp)\n",
        "    self.state=new_state\n",
        "    if is_done:\n",
        "      done_reward=self.total_reward\n",
        "      self._reset()\n",
        "    return done_reward\n",
        "\n",
        "#计算损失\n",
        "def calc_loss(batch,net,tgt_net,device='cpu'):\n",
        "  states,actions,rewards,dones,new_states=batch\n",
        "  #将numpy array 放入gpu中进行计算\n",
        "  states_v=torch.tensor(states).to(device)\n",
        "  actions_v=torch.tensor(actions).to(device)\n",
        "  rewards_v=torch.tensor(rewards).to(device)\n",
        "  next_states_v=torch.tensor(new_states).to(device)\n",
        "  done_mask=torch.BoolTensor(dones).to(device)\n",
        "\n",
        "  #计算Q值和target\n",
        "  state_action_values=net(states_v).gather(1,actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "  next_states_values=tgt_net(next_states_v).max(1)[0]\n",
        "  next_states_values[done_mask]=0.0#对于已经完成的就算0\n",
        "  next_states_values=next_states_values.detach()#不进行梯度更新\n",
        "  expected_state_action_value=next_states_values*gamma+rewards_v#Qlearning的更新规则\n",
        "  return nn.MSELoss()(state_action_values,expected_state_action_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d5wDFh3PMiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h2nLaLtJ1LD",
        "colab_type": "text"
      },
      "source": [
        "### 开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoUF1W7iJz6W",
        "colab_type": "code",
        "outputId": "31300f91-4a28-4c8c-aa0e-ff9003499e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "import time\n",
        "\n",
        "#环境名字\n",
        "env_name=\"PongNoFrameskip-v4\"\n",
        "mean_reward_bound=19.5\n",
        "device='cuda'\n",
        "\n",
        "#超参数\n",
        "\n",
        "replay_size=10000\n",
        "epsilon_decay_last_frame=10**5\n",
        "epsilon_start=1.0\n",
        "epsilon_final=0.02\n",
        "replay_start_size=10000\n",
        "learning_rate=1e-4\n",
        "sync_target_frame=1000\n",
        "mean_reward_bound=19.5\n",
        "batch_size=32\n",
        "\n",
        "#环境与网络\n",
        "\n",
        "env=make_env(env_name)\n",
        "\n",
        "net=DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "tgt_net=DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "\n",
        "writer=SummaryWriter(comment='-')\n",
        "buffer=ExperienceBuffer(replay_size)\n",
        "\n",
        "agent=Agent(env,buffer)\n",
        "epsilon=epsilon_start\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(),lr=learning_rate)\n",
        "total_rewards=[]\n",
        "frame_idx=0\n",
        "ts_frame=0\n",
        "ts=time.time()\n",
        "best_mean_reward=None\n",
        "\n",
        "\n",
        "#开始循环\n",
        "while True:\n",
        "  frame_idx+=1\n",
        "  #epsilon线性递减\n",
        "  epsilon=max(epsilon_final,epsilon_start-frame_idx/epsilon_decay_last_frame)\n",
        "  reward = agent.play_step(net, epsilon, device=device)#buffer的操作在agent里面\n",
        "  if reward is not None:\n",
        "    total_rewards.append(reward)\n",
        "    speed = (frame_idx - ts_frame) / (time.time() - ts)#计算每秒能处理多少帧\n",
        "    ts_frame = frame_idx\n",
        "    ts = time.time()\n",
        "    mean_reward = np.mean(total_rewards[-100:])#表示最后100帧的平均回报\n",
        "    print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "    writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "    writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "    writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "    writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "    #存储最好的模型\n",
        "    if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "      torch.save(net.state_dict(),env_name+\"-best.dat\")\n",
        "      if best_mean_reward is not None:\n",
        "         print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "         best_mean_reward=mean_reward\n",
        "    if mean_reward > mean_reward_bound:\n",
        "      print(\"Solved in %d frames!\" % frame_idx)\n",
        "      break;\n",
        "  #只有replay buffer 满了才进行梯度更新\n",
        "  if len(buffer) < replay_start_size:\n",
        "    continue\n",
        "  if frame_idx % sync_target_frame == 0:\n",
        "    tgt_net.load_state_dict(net.state_dict())\n",
        "  optimizer.zero_grad()\n",
        "  batch = buffer.sample(batch_size)\n",
        "  loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "  loss_t.backward()\n",
        "  optimizer.step()\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "910: done 1 games, mean reward -21.000, eps 0.99, speed 681.07 f/s\n",
            "2035: done 2 games, mean reward -20.500, eps 0.98, speed 663.23 f/s\n",
            "2881: done 3 games, mean reward -20.667, eps 0.97, speed 666.95 f/s\n",
            "3751: done 4 games, mean reward -20.750, eps 0.96, speed 648.14 f/s\n",
            "4636: done 5 games, mean reward -20.800, eps 0.95, speed 650.37 f/s\n",
            "5504: done 6 games, mean reward -20.667, eps 0.94, speed 651.86 f/s\n",
            "6402: done 7 games, mean reward -20.571, eps 0.94, speed 654.90 f/s\n",
            "7273: done 8 games, mean reward -20.625, eps 0.93, speed 641.60 f/s\n",
            "8095: done 9 games, mean reward -20.667, eps 0.92, speed 642.82 f/s\n",
            "8875: done 10 games, mean reward -20.700, eps 0.91, speed 652.94 f/s\n",
            "9725: done 11 games, mean reward -20.727, eps 0.90, speed 646.77 f/s\n",
            "10547: done 12 games, mean reward -20.750, eps 0.89, speed 143.90 f/s\n",
            "11388: done 13 games, mean reward -20.769, eps 0.89, speed 104.27 f/s\n",
            "12298: done 14 games, mean reward -20.786, eps 0.88, speed 104.38 f/s\n",
            "13200: done 15 games, mean reward -20.733, eps 0.87, speed 102.16 f/s\n",
            "14256: done 16 games, mean reward -20.625, eps 0.86, speed 104.24 f/s\n",
            "15138: done 17 games, mean reward -20.647, eps 0.85, speed 104.37 f/s\n",
            "16147: done 18 games, mean reward -20.611, eps 0.84, speed 102.16 f/s\n",
            "16937: done 19 games, mean reward -20.632, eps 0.83, speed 98.87 f/s\n",
            "17912: done 20 games, mean reward -20.600, eps 0.82, speed 103.26 f/s\n",
            "18674: done 21 games, mean reward -20.619, eps 0.81, speed 102.80 f/s\n",
            "19496: done 22 games, mean reward -20.636, eps 0.81, speed 101.85 f/s\n",
            "20286: done 23 games, mean reward -20.652, eps 0.80, speed 103.89 f/s\n",
            "21138: done 24 games, mean reward -20.667, eps 0.79, speed 103.38 f/s\n",
            "21956: done 25 games, mean reward -20.680, eps 0.78, speed 102.00 f/s\n",
            "22858: done 26 games, mean reward -20.692, eps 0.77, speed 102.21 f/s\n",
            "23718: done 27 games, mean reward -20.667, eps 0.76, speed 102.56 f/s\n",
            "24676: done 28 games, mean reward -20.643, eps 0.75, speed 101.79 f/s\n",
            "25916: done 29 games, mean reward -20.655, eps 0.74, speed 100.32 f/s\n",
            "26924: done 30 games, mean reward -20.633, eps 0.73, speed 102.48 f/s\n",
            "27747: done 31 games, mean reward -20.645, eps 0.72, speed 101.52 f/s\n",
            "28844: done 32 games, mean reward -20.625, eps 0.71, speed 101.50 f/s\n",
            "29634: done 33 games, mean reward -20.636, eps 0.70, speed 101.61 f/s\n",
            "30564: done 34 games, mean reward -20.647, eps 0.69, speed 102.43 f/s\n",
            "31404: done 35 games, mean reward -20.629, eps 0.69, speed 100.07 f/s\n",
            "32307: done 36 games, mean reward -20.639, eps 0.68, speed 99.55 f/s\n",
            "33377: done 37 games, mean reward -20.622, eps 0.67, speed 100.38 f/s\n",
            "34632: done 38 games, mean reward -20.553, eps 0.65, speed 99.93 f/s\n",
            "35689: done 39 games, mean reward -20.538, eps 0.64, speed 100.71 f/s\n",
            "36571: done 40 games, mean reward -20.550, eps 0.63, speed 101.71 f/s\n",
            "37630: done 41 games, mean reward -20.488, eps 0.62, speed 99.44 f/s\n",
            "38452: done 42 games, mean reward -20.500, eps 0.62, speed 100.49 f/s\n",
            "39494: done 43 games, mean reward -20.465, eps 0.61, speed 100.58 f/s\n",
            "40466: done 44 games, mean reward -20.477, eps 0.60, speed 100.33 f/s\n",
            "41499: done 45 games, mean reward -20.422, eps 0.59, speed 98.41 f/s\n",
            "42309: done 46 games, mean reward -20.435, eps 0.58, speed 100.74 f/s\n",
            "43454: done 47 games, mean reward -20.362, eps 0.57, speed 99.58 f/s\n",
            "44244: done 48 games, mean reward -20.375, eps 0.56, speed 99.62 f/s\n",
            "45265: done 49 games, mean reward -20.367, eps 0.55, speed 89.86 f/s\n",
            "46252: done 50 games, mean reward -20.380, eps 0.54, speed 94.40 f/s\n",
            "47033: done 51 games, mean reward -20.392, eps 0.53, speed 92.99 f/s\n",
            "48036: done 52 games, mean reward -20.385, eps 0.52, speed 91.29 f/s\n",
            "49214: done 53 games, mean reward -20.340, eps 0.51, speed 93.81 f/s\n",
            "50549: done 54 games, mean reward -20.278, eps 0.49, speed 97.47 f/s\n",
            "51671: done 55 games, mean reward -20.255, eps 0.48, speed 97.85 f/s\n",
            "52861: done 56 games, mean reward -20.232, eps 0.47, speed 98.43 f/s\n",
            "54117: done 57 games, mean reward -20.228, eps 0.46, speed 95.80 f/s\n",
            "55298: done 58 games, mean reward -20.207, eps 0.45, speed 95.14 f/s\n",
            "56585: done 59 games, mean reward -20.153, eps 0.43, speed 96.06 f/s\n",
            "57945: done 60 games, mean reward -20.117, eps 0.42, speed 96.57 f/s\n",
            "59433: done 61 games, mean reward -20.115, eps 0.41, speed 97.15 f/s\n",
            "60883: done 62 games, mean reward -20.097, eps 0.39, speed 96.45 f/s\n",
            "62423: done 63 games, mean reward -20.032, eps 0.38, speed 95.92 f/s\n",
            "63874: done 64 games, mean reward -20.000, eps 0.36, speed 95.79 f/s\n",
            "65604: done 65 games, mean reward -19.985, eps 0.34, speed 96.61 f/s\n",
            "67012: done 66 games, mean reward -19.985, eps 0.33, speed 95.26 f/s\n",
            "68451: done 67 games, mean reward -20.000, eps 0.32, speed 96.76 f/s\n",
            "69741: done 68 games, mean reward -20.000, eps 0.30, speed 95.14 f/s\n",
            "71366: done 69 games, mean reward -19.942, eps 0.29, speed 95.36 f/s\n",
            "73529: done 70 games, mean reward -19.871, eps 0.26, speed 94.81 f/s\n",
            "75415: done 71 games, mean reward -19.873, eps 0.25, speed 94.93 f/s\n",
            "76914: done 72 games, mean reward -19.875, eps 0.23, speed 95.99 f/s\n",
            "78381: done 73 games, mean reward -19.863, eps 0.22, speed 93.18 f/s\n",
            "79488: done 74 games, mean reward -19.878, eps 0.21, speed 95.78 f/s\n",
            "80958: done 75 games, mean reward -19.853, eps 0.19, speed 94.42 f/s\n",
            "82641: done 76 games, mean reward -19.776, eps 0.17, speed 95.39 f/s\n",
            "83965: done 77 games, mean reward -19.753, eps 0.16, speed 94.13 f/s\n",
            "86052: done 78 games, mean reward -19.718, eps 0.14, speed 94.19 f/s\n",
            "88041: done 79 games, mean reward -19.646, eps 0.12, speed 93.03 f/s\n",
            "90144: done 80 games, mean reward -19.600, eps 0.10, speed 93.10 f/s\n",
            "92038: done 81 games, mean reward -19.568, eps 0.08, speed 92.77 f/s\n",
            "94015: done 82 games, mean reward -19.500, eps 0.06, speed 92.37 f/s\n",
            "95865: done 83 games, mean reward -19.470, eps 0.04, speed 91.61 f/s\n",
            "97747: done 84 games, mean reward -19.452, eps 0.02, speed 90.85 f/s\n",
            "99890: done 85 games, mean reward -19.412, eps 0.02, speed 91.55 f/s\n",
            "102358: done 86 games, mean reward -19.360, eps 0.02, speed 91.48 f/s\n",
            "105124: done 87 games, mean reward -19.299, eps 0.02, speed 91.37 f/s\n",
            "107382: done 88 games, mean reward -19.250, eps 0.02, speed 90.65 f/s\n",
            "110091: done 89 games, mean reward -19.169, eps 0.02, speed 91.11 f/s\n",
            "112369: done 90 games, mean reward -19.133, eps 0.02, speed 91.45 f/s\n",
            "114712: done 91 games, mean reward -19.132, eps 0.02, speed 91.64 f/s\n",
            "118025: done 92 games, mean reward -19.022, eps 0.02, speed 91.10 f/s\n",
            "121065: done 93 games, mean reward -18.946, eps 0.02, speed 84.43 f/s\n",
            "123507: done 94 games, mean reward -18.915, eps 0.02, speed 83.67 f/s\n",
            "126386: done 95 games, mean reward -18.863, eps 0.02, speed 82.86 f/s\n",
            "129731: done 96 games, mean reward -18.812, eps 0.02, speed 82.76 f/s\n",
            "132661: done 97 games, mean reward -18.763, eps 0.02, speed 83.31 f/s\n",
            "136203: done 98 games, mean reward -18.643, eps 0.02, speed 85.12 f/s\n",
            "139492: done 99 games, mean reward -18.545, eps 0.02, speed 86.92 f/s\n",
            "143042: done 100 games, mean reward -18.510, eps 0.02, speed 92.81 f/s\n",
            "146527: done 101 games, mean reward -18.420, eps 0.02, speed 92.98 f/s\n",
            "149890: done 102 games, mean reward -18.360, eps 0.02, speed 93.02 f/s\n",
            "154002: done 103 games, mean reward -18.200, eps 0.02, speed 92.25 f/s\n",
            "156741: done 104 games, mean reward -18.150, eps 0.02, speed 92.70 f/s\n",
            "160747: done 105 games, mean reward -18.040, eps 0.02, speed 92.48 f/s\n",
            "163309: done 106 games, mean reward -18.030, eps 0.02, speed 91.45 f/s\n",
            "167091: done 107 games, mean reward -17.920, eps 0.02, speed 92.36 f/s\n",
            "171163: done 108 games, mean reward -17.770, eps 0.02, speed 92.76 f/s\n",
            "174120: done 109 games, mean reward -17.690, eps 0.02, speed 92.18 f/s\n",
            "176990: done 110 games, mean reward -17.630, eps 0.02, speed 91.88 f/s\n",
            "180037: done 111 games, mean reward -17.560, eps 0.02, speed 92.17 f/s\n",
            "183827: done 112 games, mean reward -17.400, eps 0.02, speed 90.83 f/s\n",
            "187504: done 113 games, mean reward -17.220, eps 0.02, speed 91.56 f/s\n",
            "190400: done 114 games, mean reward -17.110, eps 0.02, speed 90.64 f/s\n",
            "194185: done 115 games, mean reward -16.950, eps 0.02, speed 91.55 f/s\n",
            "197147: done 116 games, mean reward -16.690, eps 0.02, speed 91.08 f/s\n",
            "200161: done 117 games, mean reward -16.430, eps 0.02, speed 91.93 f/s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2lyVoiRc3UC",
        "colab_type": "text"
      },
      "source": [
        "### 运行,测试"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9tPkG7Hc1wA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "\n",
        "\n",
        "env = make_env(args.env)\n",
        "env = gym.wrappers.Monitor(env, '/tmp')\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "c = collections.Counter()\n",
        "\n",
        "while True:\n",
        "  start_ts = time.time()\n",
        "  state_v = torch.tensor(np.array([state], copy=False))\n",
        "  q_vals = net(state_v).data.numpy()[0]\n",
        "  action = np.argmax(q_vals)\n",
        "  c[action] += 1\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  total_reward += reward\n",
        "  if done:\n",
        "    break\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKvdGVfgcp76",
        "colab_type": "text"
      },
      "source": [
        "### python 几个函数详解\n",
        "\n",
        "##### 1、卷积神经网络的参数计算公式\n",
        "\n",
        "```\n",
        "N=(W-F+2P)/S+1\n",
        "```\n",
        "其中W表示输入图片的大小，F表示卷积核大小，P表示padding的像素\n",
        "S表示stride\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZToUL9CMM275",
        "colab_type": "text"
      },
      "source": [
        "### 与上面无关，华为云obs管理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYBHmW9LHuzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://cnnorth1-modelarts-sdk.obs.cn-north-1.myhwclouds.com/modelarts-1.1.3-py2.py3-none-any.whl\n",
        "!pip install modelarts-1.1.3-py2.py3-none-any.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u44IN1IAJo4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from modelarts.session import Session\n",
        "session = Session(access_key='5QEDGVWFKVRYBE3TW0ND',secret_key='l9d7DkG9Nf5cBLxalMjqWRAcU4Q1OIosaieKPC4R', project_id='079c52e0430026042f71c004ba7d3ceb', region_name='cn-north-4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZisivZnhLssa",
        "colab_type": "code",
        "outputId": "108b5b16-552c-4429-ee6d-e838992bb0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "session.download_data(bucket_path=\"/aistart/obs aifood baseline codes/\", path=\"/content/drive/My Drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully download file aistart/obs aifood baseline codes from OBS to local /content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}