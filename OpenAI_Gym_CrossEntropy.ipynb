{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI_Gym_CrossEntropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS5cx2WbwL02sNxoUftJPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zll134/deeplearning-tutorial/blob/master/OpenAI_Gym_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJhYPyQpjEM8",
        "colab_type": "text"
      },
      "source": [
        "导入必要的包，是openai gym能够在colab上运行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-1K8Amd1CLF",
        "colab_type": "code",
        "outputId": "e776d71d-a945-4360-83f6-3c6a3e594240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#安装必要的包\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SrG9m4WLzaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入需要的包\n",
        "#Open_AI所需要的包\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "#神经网络需要的包\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "#ipythondisplay.clear_output()#清除输出\n",
        "#print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLLK1wv5Xaxl",
        "colab_type": "text"
      },
      "source": [
        "### OpenAI gym的使用\n",
        "#### env类\n",
        "env类有四个成员：<br>\n",
        "- action_space：<br>\n",
        "- observation_space<br>\n",
        "- reset():该方法表示将环境初始化<br>\n",
        "- step()：当action作为参数输入是，以python元组的形式返回四个信息分别是:\n",
        "    - observation：返回的是numpy向量\n",
        "    - reward：返回的是以个浮点数\n",
        "    - done：如果为true则说明episode结束\n",
        "    - extra_info：\n",
        "    \n",
        "创建环境（传统控制中的木杆平衡）:\n",
        "木杆控制环境的观察空间为木杆中心的x坐标，以及x方向上的速度，以及木杆的旋转角度和旋转速度\n",
        "\n",
        "### 使用交叉熵的方法进行强化学习\n",
        "\n",
        "步骤：\n",
        "1. 使用当前模型和环境进行N段episode的play\n",
        "2. 计算N段episode的总reward。\n",
        "3. 取前50%或者30%的episode进行训练。\n",
        "4. 重复前面的步骤\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXxo02wxRvB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4969fea1-9a44-4ade-b653-8cf2af77284b"
      },
      "source": [
        "hidden_size=128\n",
        "batch_size=16\n",
        "percentile=70\n",
        "\n",
        "#我们要定义一个三层的神经网络，一个输入层，一个输出层和一个隐藏层,应该叫做mlp\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,obs_size,hidden_size,n_action):\n",
        "    super(Net,self).__init__();\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(obs_size,hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,n_action)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "#两个辅助变量,用于将训\n",
        "Episode=namedtuple('episode',['reward','steps'])#存储总的reward和动作集合\n",
        "EpisodeStep=namedtuple('EpisodeStep',['observation','action'])#存储每一步的观察与动作\n",
        "\n",
        "#定义一个迭代函数,tongguo episode生成批\n",
        "def iteration_batches(env,net,batch_size):\n",
        "  batch=[];\n",
        "  episode_reward=0.0;\n",
        "  episode_steps=[];\n",
        "  obs=env.reset();\n",
        "  sm=nn.Softmax();\n",
        "  while True:\n",
        "    #完成一次动作\n",
        "    obs_v=torch.FloatTensor([obs]);\n",
        "    act_probs_v=sm(net(obs_v));\n",
        "    act_probs=act_probs_v.data.numpy()[0];\n",
        "    action=np.random.choice(len(act_probs),p=act_probs);\n",
        "    next_obs,reward,is_done,_=env.step(action)\n",
        "\n",
        "    #存储一次动作的结果\n",
        "    episode_reward+=reward;\n",
        "    episode_steps.append(EpisodeStep(observation=obs,action=action));\n",
        "\n",
        "    #如果一个episode结束就要采取下列动作\n",
        "    if is_done:\n",
        "      batch.append(Episode(reward=episode_reward,steps=episode_steps));\n",
        "      episode_reward=0.0;\n",
        "      episode_steps=[];\n",
        "      next_obs=env.reset();\n",
        "      if len(batch)==batch_size:\n",
        "        yield batch#yield 表示函数下次从这个地方开始执行,在训练的时候表现他的作用\n",
        "        batch=[]\n",
        "    obs=next_obs\n",
        "\n",
        "#筛选出，其中回报较高的episode，构成训练集\n",
        "def filter_batch(batch,percentile):\n",
        "  rewards=list(map(lambda s:s.reward,batch));\n",
        "  reward_bound=np.percentile(rewards,percentile);\n",
        "  reward_mean=float(np.mean(rewards));\n",
        "  train_obs=[]\n",
        "  train_act=[]\n",
        "  for example in batch:\n",
        "    if example.reward<reward_bound:\n",
        "      continue;\n",
        "    train_obs.extend(list(map(lambda step:step.observation,example.steps)));\n",
        "    train_act.extend(list(map(lambda step:step.action,example.steps)));\n",
        "  train_obs_v=torch.FloatTensor(train_obs);\n",
        "  train_act_v=torch.LongTensor(train_act);\n",
        "  return train_obs_v,train_act_v,reward_bound,reward_mean\n",
        "\n",
        "#清除输出\n",
        "#ipythondisplay.clear_output()\n",
        "print('Done')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWhC4Bv_nBm9",
        "colab_type": "text"
      },
      "source": [
        "开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyitfjcQL-63",
        "colab_type": "code",
        "outputId": "e948c11d-2f1d-46c4-db45-4ebe0f8b52d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#开始训练\n",
        "env = gym.make(\"CartPole-v0\")#env是一个gym的env类\n",
        "obs_size=env.observation_space.shape[0]\n",
        "n_action=env.action_space.n;\n",
        "\n",
        "#目标\n",
        "net=Net(obs_size,hidden_size,n_action)\n",
        "objective=nn.CrossEntropyLoss();#目标函数\n",
        "optimizer=optim.Adam(params=net.parameters(),lr=0.01)#优化器\n",
        "writer=SummaryWriter()\n",
        "\n",
        "#开始训练\n",
        "for iter_no,batch in enumerate(iteration_batches(env,net,batch_size)):\n",
        "  obs_v,act_v,reward_b,reward_m=filter_batch(batch,percentile);\n",
        "  optimizer.zero_grad();\n",
        "  action_score_v=net(obs_v);\n",
        "  loss_v=objective(action_score_v,act_v);\n",
        "  loss_v.backward();\n",
        "  print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "            iter_no, loss_v.item(), reward_m, reward_b))\n",
        "  optimizer.step();\n",
        "  writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "  writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "  writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "  if reward_m > 199:\n",
        "    print(\"Solved!\")\n",
        "    break\n",
        "writer.close()\n",
        "#清除输出\n",
        "#ipythondisplay.clear_output()\n",
        "print('Done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.687, reward_mean=25.8, reward_bound=27.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: loss=0.689, reward_mean=18.8, reward_bound=19.5\n",
            "2: loss=0.687, reward_mean=28.6, reward_bound=32.0\n",
            "3: loss=0.675, reward_mean=35.8, reward_bound=41.0\n",
            "4: loss=0.666, reward_mean=31.7, reward_bound=31.5\n",
            "5: loss=0.647, reward_mean=37.2, reward_bound=41.0\n",
            "6: loss=0.652, reward_mean=50.5, reward_bound=52.0\n",
            "7: loss=0.635, reward_mean=35.1, reward_bound=35.5\n",
            "8: loss=0.618, reward_mean=66.1, reward_bound=75.0\n",
            "9: loss=0.625, reward_mean=57.8, reward_bound=58.5\n",
            "10: loss=0.611, reward_mean=64.9, reward_bound=76.5\n",
            "11: loss=0.610, reward_mean=67.4, reward_bound=75.5\n",
            "12: loss=0.604, reward_mean=47.1, reward_bound=56.0\n",
            "13: loss=0.597, reward_mean=55.4, reward_bound=74.5\n",
            "14: loss=0.600, reward_mean=68.6, reward_bound=87.0\n",
            "15: loss=0.589, reward_mean=69.8, reward_bound=76.5\n",
            "16: loss=0.576, reward_mean=87.1, reward_bound=100.0\n",
            "17: loss=0.582, reward_mean=96.0, reward_bound=116.5\n",
            "18: loss=0.570, reward_mean=112.2, reward_bound=136.5\n",
            "19: loss=0.581, reward_mean=99.6, reward_bound=120.0\n",
            "20: loss=0.560, reward_mean=102.4, reward_bound=122.0\n",
            "21: loss=0.571, reward_mean=95.6, reward_bound=111.0\n",
            "22: loss=0.556, reward_mean=110.9, reward_bound=120.5\n",
            "23: loss=0.561, reward_mean=99.8, reward_bound=110.0\n",
            "24: loss=0.557, reward_mean=93.8, reward_bound=103.0\n",
            "25: loss=0.550, reward_mean=94.8, reward_bound=105.5\n",
            "26: loss=0.556, reward_mean=115.9, reward_bound=122.5\n",
            "27: loss=0.532, reward_mean=130.4, reward_bound=150.5\n",
            "28: loss=0.520, reward_mean=114.9, reward_bound=124.5\n",
            "29: loss=0.544, reward_mean=132.3, reward_bound=151.0\n",
            "30: loss=0.548, reward_mean=127.2, reward_bound=142.5\n",
            "31: loss=0.523, reward_mean=137.2, reward_bound=153.5\n",
            "32: loss=0.512, reward_mean=113.9, reward_bound=136.5\n",
            "33: loss=0.513, reward_mean=133.6, reward_bound=147.5\n",
            "34: loss=0.543, reward_mean=94.9, reward_bound=106.5\n",
            "35: loss=0.529, reward_mean=130.1, reward_bound=142.0\n",
            "36: loss=0.535, reward_mean=124.8, reward_bound=139.0\n",
            "37: loss=0.514, reward_mean=132.2, reward_bound=148.5\n",
            "38: loss=0.523, reward_mean=138.2, reward_bound=168.5\n",
            "39: loss=0.500, reward_mean=148.6, reward_bound=158.5\n",
            "40: loss=0.513, reward_mean=151.1, reward_bound=163.0\n",
            "41: loss=0.510, reward_mean=156.2, reward_bound=182.5\n",
            "42: loss=0.496, reward_mean=156.1, reward_bound=193.5\n",
            "43: loss=0.490, reward_mean=166.9, reward_bound=200.0\n",
            "44: loss=0.477, reward_mean=156.9, reward_bound=179.5\n",
            "45: loss=0.498, reward_mean=167.9, reward_bound=200.0\n",
            "46: loss=0.480, reward_mean=185.2, reward_bound=200.0\n",
            "47: loss=0.476, reward_mean=176.4, reward_bound=200.0\n",
            "48: loss=0.507, reward_mean=171.1, reward_bound=200.0\n",
            "49: loss=0.503, reward_mean=174.2, reward_bound=200.0\n",
            "50: loss=0.497, reward_mean=192.6, reward_bound=200.0\n",
            "51: loss=0.490, reward_mean=169.2, reward_bound=200.0\n",
            "52: loss=0.497, reward_mean=185.8, reward_bound=200.0\n",
            "53: loss=0.507, reward_mean=194.9, reward_bound=200.0\n",
            "54: loss=0.491, reward_mean=193.3, reward_bound=200.0\n",
            "55: loss=0.497, reward_mean=194.3, reward_bound=200.0\n",
            "56: loss=0.506, reward_mean=193.3, reward_bound=200.0\n",
            "57: loss=0.501, reward_mean=200.0, reward_bound=200.0\n",
            "Solved!\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG",
        "colab_type": "code",
        "outputId": "5cad7825-c878-44a6-da1e-605660b1a455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1087'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1087'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEUj1kDCns7d",
        "colab_type": "text"
      },
      "source": [
        "开始测试，观察算法的运行情况"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apUb-dAVn2wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm=nn.Softmax();\n",
        "obs=env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "while True:\n",
        "  \n",
        "  #做出动作\n",
        "  obs_v=torch.FloatTensor(obs)\n",
        "  act_probs_v=sm(net(obs_v))\n",
        "  act_probs=act_probs_v.data.numpy()\n",
        "  print(act_probs)\n",
        "  action=np.random.choice(len(act_probs),p=act_probs)\n",
        "  next_obs,reward,is_done,_=env.step(action)\n",
        "\n",
        "  #显示图形显示\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "  #迭代更新\n",
        "  obs=next_obs\n",
        "  if is_done:\n",
        "    break;\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}