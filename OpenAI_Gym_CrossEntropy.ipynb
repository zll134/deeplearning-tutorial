{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI_Gym_CrossEntropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOS+m232/dGtspjoHuA54d/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zll134/deeplearning-tutorial/blob/master/OpenAI_Gym_CrossEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJhYPyQpjEM8",
        "colab_type": "text"
      },
      "source": [
        "导入必要的包，是openai gym能够在colab上运行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-1K8Amd1CLF",
        "colab_type": "code",
        "outputId": "0479adf6-0ca1-4d65-ad52-aa670b202f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "#安装必要的包\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SrG9m4WLzaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 导入需要的包\n",
        "#Open_AI所需要的包\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "#神经网络需要的包\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "#ipythondisplay.clear_output()#清除输出\n",
        "#print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLLK1wv5Xaxl",
        "colab_type": "text"
      },
      "source": [
        "### OpenAI gym的使用\n",
        "#### env类\n",
        "env类有四个成员：<br>\n",
        "- action_space：<br>\n",
        "- observation_space<br>\n",
        "- reset():该方法表示将环境初始化<br>\n",
        "- step()：当action作为参数输入是，以python元组的形式返回四个信息分别是:\n",
        "    - observation：返回的是numpy向量\n",
        "    - reward：返回的是以个浮点数\n",
        "    - done：如果为true则说明episode结束\n",
        "    - extra_info：\n",
        "    \n",
        "创建环境（传统控制中的木杆平衡）:\n",
        "木杆控制环境的观察空间为木杆中心的x坐标，以及x方向上的速度，以及木杆的旋转角度和旋转速度\n",
        "\n",
        "### 使用交叉熵的方法进行强化学习\n",
        "\n",
        "步骤：\n",
        "1. 使用当前模型和环境进行N段episode的play\n",
        "2. 计算N段episode的总reward。\n",
        "3. 取前50%或者30%的episode进行训练。\n",
        "4. 重复前面的步骤\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXxo02wxRvB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bef71e61-bfd1-4b17-d3b8-95ad6194dba3"
      },
      "source": [
        "hidden_size=128\n",
        "batch_size=16\n",
        "percentile=70\n",
        "\n",
        "#我们要定义一个三层的神经网络，一个输入层，一个输出层和一个隐藏层,应该叫做mlp\n",
        "class Net(nn.Module):\n",
        "  def __init__(self,obs_size,hidden_size,n_action):\n",
        "    super(Net,self).__init__();\n",
        "    self.net=nn.Sequential(\n",
        "        nn.Linear(obs_size,hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,n_action)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "#两个辅助变量,用于将训\n",
        "Episode=namedtuple('episode',['reward','steps'])#存储总的reward和动作集合\n",
        "EpisodeStep=namedtuple('EpisodeStep',['observation','action'])#存储每一步的观察与动作\n",
        "\n",
        "#定义一个迭代函数,tongguo episode生成批\n",
        "def iteration_batches(env,net,batch_size):\n",
        "  batch=[];\n",
        "  episode_reward=0.0;\n",
        "  episode_steps=[];\n",
        "  obs=env.reset();\n",
        "  sm=nn.Softmax();\n",
        "  while True:\n",
        "    #完成一次动作\n",
        "    obs_v=torch.FloatTensor([obs]);\n",
        "    act_probs_v=sm(net(obs_v));\n",
        "    act_probs=act_probs_v.data.numpy()[0];\n",
        "    action=np.random.choice(len(act_probs),p=act_probs);\n",
        "    next_obs,reward,is_done,_=env.step(action)\n",
        "\n",
        "    #存储一次动作的结果\n",
        "    episode_reward+=reward;\n",
        "    episode_steps.append(EpisodeStep(observation=obs,action=action));\n",
        "\n",
        "    #如果一个episode结束就要采取下列动作\n",
        "    if is_done:\n",
        "      batch.append(Episode(reward=episode_reward,steps=episode_steps));\n",
        "      episode_reward=0.0;\n",
        "      episode_steps=[];\n",
        "      next_obs=env.reset();\n",
        "      if len(batch)==batch_size:\n",
        "        yield batch#yield 表示函数下次从这个地方开始执行,在训练的时候表现他的作用\n",
        "        batch=[]\n",
        "    obs=next_obs\n",
        "\n",
        "#筛选出，其中回报较高的episode，构成训练集\n",
        "def filter_batch(batch,percentile):\n",
        "  rewards=list(map(lambda s:s.reward,batch));\n",
        "  reward_bound=np.percentile(rewards,percentile);\n",
        "  reward_mean=float(np.mean(rewards));\n",
        "  train_obs=[]\n",
        "  train_act=[]\n",
        "  for example in batch:\n",
        "    if example.reward<reward_bound:\n",
        "      continue;\n",
        "    train_obs.extend(list(map(lambda step:step.observation,example.steps)));\n",
        "    train_act.extend(list(map(lambda step:step.action,example.steps)));\n",
        "  train_obs_v=torch.FloatTensor(train_obs);\n",
        "  train_act_v=torch.LongTensor(train_act);\n",
        "  return train_obs_v,train_act_v,reward_bound,reward_mean\n",
        "\n",
        "#清除输出\n",
        "#ipythondisplay.clear_output()\n",
        "print('Done')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWhC4Bv_nBm9",
        "colab_type": "text"
      },
      "source": [
        "开始训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyitfjcQL-63",
        "colab_type": "code",
        "outputId": "e3175836-4df9-4a0a-c878-70d97e83bcfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#开始训练\n",
        "env = gym.make(\"CartPole-v0\")#env是一个gym的env类\n",
        "obs_size=env.observation_space.shape[0]\n",
        "n_action=env.action_space.n;\n",
        "\n",
        "#目标\n",
        "net=Net(obs_size,hidden_size,n_action)\n",
        "objective=nn.CrossEntropyLoss();#目标函数\n",
        "optimizer=optim.Adam(params=net.parameters(),lr=0.01)#优化器\n",
        "writer=SummaryWriter()\n",
        "\n",
        "#开始训练\n",
        "for iter_no,batch in enumerate(iteration_batches(env,net,batch_size)):\n",
        "  obs_v,act_v,reward_b,reward_m=filter_batch(batch,percentile);\n",
        "  optimizer.zero_grad();\n",
        "  action_score_v=net(obs_v);\n",
        "  loss_v=objective(action_score_v,act_v);\n",
        "  loss_v.backward();\n",
        "  print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "            iter_no, loss_v.item(), reward_m, reward_b))\n",
        "  optimizer.step();\n",
        "  writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "  writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "  writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "  if reward_m > 199:\n",
        "    print(\"Solved!\")\n",
        "    break\n",
        "writer.close()\n",
        "#清除输出\n",
        "#ipythondisplay.clear_output()\n",
        "print('Done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.702, reward_mean=20.4, reward_bound=22.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1: loss=0.680, reward_mean=27.6, reward_bound=31.0\n",
            "2: loss=0.674, reward_mean=23.2, reward_bound=29.5\n",
            "3: loss=0.684, reward_mean=26.2, reward_bound=30.0\n",
            "4: loss=0.686, reward_mean=21.8, reward_bound=21.5\n",
            "5: loss=0.658, reward_mean=28.9, reward_bound=34.0\n",
            "6: loss=0.654, reward_mean=30.4, reward_bound=37.0\n",
            "7: loss=0.651, reward_mean=44.2, reward_bound=60.5\n",
            "8: loss=0.634, reward_mean=40.9, reward_bound=43.0\n",
            "9: loss=0.627, reward_mean=40.8, reward_bound=41.0\n",
            "10: loss=0.629, reward_mean=35.9, reward_bound=42.0\n",
            "11: loss=0.609, reward_mean=43.0, reward_bound=50.5\n",
            "12: loss=0.627, reward_mean=39.9, reward_bound=49.5\n",
            "13: loss=0.603, reward_mean=38.3, reward_bound=47.0\n",
            "14: loss=0.595, reward_mean=47.2, reward_bound=55.0\n",
            "15: loss=0.599, reward_mean=47.5, reward_bound=56.5\n",
            "16: loss=0.586, reward_mean=60.9, reward_bound=73.0\n",
            "17: loss=0.572, reward_mean=53.9, reward_bound=58.0\n",
            "18: loss=0.576, reward_mean=55.8, reward_bound=61.5\n",
            "19: loss=0.564, reward_mean=52.1, reward_bound=60.5\n",
            "20: loss=0.559, reward_mean=66.8, reward_bound=77.0\n",
            "21: loss=0.551, reward_mean=55.1, reward_bound=57.0\n",
            "22: loss=0.565, reward_mean=66.3, reward_bound=81.5\n",
            "23: loss=0.542, reward_mean=57.9, reward_bound=71.0\n",
            "24: loss=0.538, reward_mean=73.4, reward_bound=84.5\n",
            "25: loss=0.549, reward_mean=71.7, reward_bound=77.5\n",
            "26: loss=0.540, reward_mean=73.2, reward_bound=87.5\n",
            "27: loss=0.521, reward_mean=65.1, reward_bound=70.5\n",
            "28: loss=0.531, reward_mean=76.2, reward_bound=90.0\n",
            "29: loss=0.509, reward_mean=74.0, reward_bound=79.5\n",
            "30: loss=0.526, reward_mean=77.3, reward_bound=96.5\n",
            "31: loss=0.526, reward_mean=75.0, reward_bound=80.0\n",
            "32: loss=0.517, reward_mean=77.4, reward_bound=91.0\n",
            "33: loss=0.522, reward_mean=70.4, reward_bound=83.5\n",
            "34: loss=0.502, reward_mean=73.8, reward_bound=77.0\n",
            "35: loss=0.533, reward_mean=76.7, reward_bound=86.5\n",
            "36: loss=0.521, reward_mean=78.1, reward_bound=88.5\n",
            "37: loss=0.479, reward_mean=70.8, reward_bound=79.5\n",
            "38: loss=0.514, reward_mean=78.2, reward_bound=88.5\n",
            "39: loss=0.497, reward_mean=74.9, reward_bound=80.5\n",
            "40: loss=0.518, reward_mean=74.6, reward_bound=79.0\n",
            "41: loss=0.504, reward_mean=85.9, reward_bound=98.5\n",
            "42: loss=0.500, reward_mean=96.6, reward_bound=109.0\n",
            "43: loss=0.498, reward_mean=73.6, reward_bound=74.0\n",
            "44: loss=0.493, reward_mean=77.2, reward_bound=88.0\n",
            "45: loss=0.474, reward_mean=90.2, reward_bound=97.0\n",
            "46: loss=0.476, reward_mean=85.0, reward_bound=93.5\n",
            "47: loss=0.491, reward_mean=94.2, reward_bound=109.5\n",
            "48: loss=0.477, reward_mean=93.8, reward_bound=101.0\n",
            "49: loss=0.488, reward_mean=98.1, reward_bound=103.0\n",
            "50: loss=0.512, reward_mean=95.1, reward_bound=102.0\n",
            "51: loss=0.486, reward_mean=107.2, reward_bound=110.0\n",
            "52: loss=0.483, reward_mean=106.1, reward_bound=109.0\n",
            "53: loss=0.493, reward_mean=108.8, reward_bound=121.5\n",
            "54: loss=0.475, reward_mean=109.8, reward_bound=119.0\n",
            "55: loss=0.492, reward_mean=129.2, reward_bound=155.5\n",
            "56: loss=0.504, reward_mean=121.5, reward_bound=132.0\n",
            "57: loss=0.490, reward_mean=150.8, reward_bound=173.0\n",
            "58: loss=0.496, reward_mean=157.2, reward_bound=181.0\n",
            "59: loss=0.490, reward_mean=163.4, reward_bound=200.0\n",
            "60: loss=0.501, reward_mean=184.9, reward_bound=200.0\n",
            "61: loss=0.485, reward_mean=177.7, reward_bound=200.0\n",
            "62: loss=0.500, reward_mean=184.4, reward_bound=200.0\n",
            "63: loss=0.495, reward_mean=189.9, reward_bound=200.0\n",
            "64: loss=0.499, reward_mean=197.2, reward_bound=200.0\n",
            "65: loss=0.498, reward_mean=191.5, reward_bound=200.0\n",
            "66: loss=0.504, reward_mean=189.8, reward_bound=200.0\n",
            "67: loss=0.504, reward_mean=193.8, reward_bound=200.0\n",
            "68: loss=0.501, reward_mean=200.0, reward_bound=200.0\n",
            "Solved!\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG",
        "colab_type": "code",
        "outputId": "1c3b5b18-89e0-40ad-e87d-56f53d439f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1087'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1087'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEUj1kDCns7d",
        "colab_type": "text"
      },
      "source": [
        "开始测试，观察算法的运行情况"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apUb-dAVn2wW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8c721fe9-8eaa-4d7f-c6a6-bcce80d08cdf"
      },
      "source": [
        "from gym import wrappers\n",
        "env = gym.make('CartPole-v0')\n",
        "env = wrappers.Monitor(env,'/tmp/cartpole-experiment-1')\n",
        "sm=nn.Softmax();\n",
        "obs=env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "for i in range(500):\n",
        "  \n",
        "  #做出动作\n",
        "  obs_v=torch.FloatTensor(obs)\n",
        "  act_probs_v=sm(net(obs_v))\n",
        "  act_probs=act_probs_v.data.numpy()\n",
        "  print(act_probs)\n",
        "  action=np.random.choice(len(act_probs),p=act_probs)\n",
        "  next_obs,reward,is_done,_=env.step(action)\n",
        "\n",
        "  #显示图形显示\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "  #迭代更新\n",
        "  obs=next_obs\n",
        "  if is_done:\n",
        "    break;\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npM78P0qj6ieUkGv8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1eUncHd3wYy",
        "colab_type": "text"
      },
      "source": [
        "# 新段落"
      ]
    }
  ]
}